<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Deploy an Operator via GitOps using Advanced Cluster Management</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/11/deploy-operator-gitops-using-advanced-cluster-management" /><author><name>Sahil Sethi</name></author><id>49f4b632-e0ec-4b59-a317-d7d4093edab3</id><updated>2022-07-11T07:00:00Z</updated><published>2022-07-11T07:00:00Z</published><summary type="html">&lt;p&gt;GitOps is a strict discipline: Everything you code or manage should be specified through configuration files in your Git repositories, and applied automatically through &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD pipelines&lt;/a&gt;. This article shows you how to integrate &lt;em&gt;security policies&lt;/em&gt; into GitOps so that they are applied consistently throughout your clusters. Security policies are part of &lt;a href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes"&gt;Red Hat Advanced Cluster Management for Kubernetes&lt;/a&gt;, a platform that helps developers configure and deploy applications along with other useful services such as metrics. This article also uses &lt;a href="https://www.redhat.com/en/resources/advanced-cluster-security-for-kubernetes-datasheet"&gt;Red Hat Advanced Cluster Security for Kubernetes&lt;/a&gt;. For background on Red Hat Advanced Cluster Management, read &lt;a href="https://cloud.redhat.com/blog/understanding-gitops-with-red-hat-advanced-cluster-management"&gt;Understanding GitOps with Red Hat Advanced Cluster Management&lt;/a&gt; on the Red Hat Hybrid Cloud blog.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;Before beginning the exercise in this article, you'll need to install the following technologies:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.9/html/installing/index"&gt;Red Hat OpenShift Container Platform 4.9&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.4/html/install/index"&gt;Red Hat Advanced Cluster Management for Kubernetes&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;A &lt;a href="https://github.com/sahilsethi12/acspolicy-gitops-acm"&gt;policies repository&lt;/a&gt; to deploy with Red Hat Advanced Cluster Management and Red Hat Advanced Cluster Security&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Custom labels can be used to select policies in Red Hat Advanced Cluster Security Management. To take advantage of this feature, create two clusters on the Openshift Container Platform. Assign a label with the name &lt;code&gt;env&lt;/code&gt; and the value &lt;code&gt;dev&lt;/code&gt; to one cluster, and a label with the name &lt;code&gt;env&lt;/code&gt; and the value &lt;code&gt;test&lt;/code&gt; to the other.&lt;/p&gt; &lt;h2&gt;Deploy the Subscription-Admin policy&lt;/h2&gt; &lt;p&gt;The policies repository listed in the prerequisites contains a Subscription-Admin policy. To activate it, in the Red Hat Advanced Cluster Management console, navigate to &lt;strong&gt;Governance→Create Policy&lt;/strong&gt;. Copy and paste the &lt;code&gt;policy-configure-subscription-admin-hub.yaml&lt;/code&gt; file from the policies repository into the YAML view. Change the namespace to match the namespace of your cluster, and change the user name (which the file defines as &lt;code&gt;kube:admin&lt;/code&gt;) to the username you use to log into Red Hat Advanced Cluster Management. Once you have created the policy, it will be shown in the &lt;strong&gt;Governance&lt;/strong&gt; page in the console (Figure 1).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/subscription-admin_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/subscription-admin_0.png?itok=utZRE1OM" width="1440" height="900" alt="Screenshow of the Governance page showing a policy after you successfully create it" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The Governance screen shows a policy after you successfully create it. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The Governance page shows a policy after you successfully create it.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Install the central policy&lt;/h2&gt; &lt;p&gt;Deploy Red Hat Advanced Cluster Security for Kubernetes by navigating to &lt;strong&gt;Applications→Create Application→Subscription&lt;/strong&gt; and entering the information shown in Figure 2:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The name of your application&lt;/li&gt; &lt;li&gt;The namespace where you want to install the application&lt;/li&gt; &lt;li&gt;The URL of its repository&lt;/li&gt; &lt;li&gt;Your username&lt;/li&gt; &lt;li&gt;Your access token&lt;/li&gt; &lt;/ul&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/application_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/application_1.png?itok=BaAnwSss" width="1440" height="900" alt="This picture shows the application details" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The "Create an application" screen asks for basic information. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Also on the &lt;strong&gt;Create an application&lt;/strong&gt; page, choose &lt;strong&gt;Deploy on local cluster&lt;/strong&gt; (Figure 3).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/application_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/application_2.png?itok=SOZXMP2G" width="1440" height="900" alt="This picture shows the application details" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The "Create an application" screen lets you deploy the application on a local cluster. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: The 'Create an application' page lets you choose where to deploy the application.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The application needs a central policy, which must be located in only one of its environments. The policies repository linked to above defines a central policy with the name &lt;code&gt;policy-advanced-cluster-security-central&lt;/code&gt; and places it in the &lt;code&gt;test&lt;/code&gt; environment using a &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.4/html-single/applications/index#placement-rules"&gt;placement rule&lt;/a&gt; in &lt;a href="https://github.com/sahilsethi12/acspolicy-gitops-acm/blob/main/centraldeploypolicy/policy-acs-operator-central.yaml"&gt;one of the YAML configuration files&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Click &lt;strong&gt;Save Application&lt;/strong&gt;. A successful creation takes you to the &lt;em&gt;resource topology,&lt;/em&gt; a visual representation of the resources in your deployed application, including the Subscription-Application (Figure 4).&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Application.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Application.png?itok=P0f31eSW" width="1440" height="900" alt="Shows the Application and all Manifests deployed using the application" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The application's topology shows the application's resources. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;From the menu on the left, you can choose the &lt;strong&gt;Governance&lt;/strong&gt; tab and see the new Subscription-Admin policy in effect (Figure 5).&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Governancetab.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Governancetab.png?itok=gvhNIgei" width="1440" height="900" alt="Shows the policies deployed via Manual as well as via Git " loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: The Governance screen now shows the Subscription-Admin policy. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Generate the init bundle for the cluster and deploy it via the application&lt;/h2&gt; &lt;p&gt;From the &lt;code&gt;test&lt;/code&gt; cluster, get the URL of the central endpoint. Enter this URL into your browser and log into the &lt;code&gt;stackrox&lt;/code&gt; namespace. (The policies repository assigned the name &lt;code&gt;stackrox&lt;/code&gt; because &lt;a href="https://www.stackrox.io"&gt;StackRox&lt;/a&gt; is the upstream project from which Red Hat Advanced Cluster Security for Kubernetes evolved, but you can use any name of your choice.) Your password will be picked from the secret named &lt;code&gt;central-htpasswd&lt;/code&gt; in the namespace.&lt;/p&gt; &lt;p&gt;Navigate to &lt;strong&gt;Platform Configuration→Integrations→Cluster Init Bundle→&lt;your_cluster_name&gt;&lt;/strong&gt;. Click &lt;strong&gt;generate→Download kubernetes Secret file&lt;/strong&gt;. Replace the automatically generated file with the &lt;a href="https://github.com/sahilsethi12/acspolicy-gitops-acm/blob/main/centralSecrets/testcluster-cluster-init-secrets-3.yaml"&gt;contents of this file&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Now you have to create an application to deploy the generated secret file in both the &lt;code&gt;dev&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; clusters. Follow these steps for each cluster:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Navigate to &lt;strong&gt;Applications→Create Application→Subscription&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Enter the name of the application and the namespace where you want to install it. Choose &lt;code&gt;centralSecrets&lt;/code&gt; as the Git path.&lt;/li&gt; &lt;li&gt;Instead of choosing a local cluster, choose the labels for which the cluster needs to be deployed. As shown in Figure 6, one label has the name &lt;code&gt;env&lt;/code&gt; with the value &lt;code&gt;test&lt;/code&gt;, and the other has the name &lt;code&gt;env&lt;/code&gt; with the value &lt;code&gt;dev&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/InitBundle_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/InitBundle_0.png?itok=Zn1DTKnl" width="1440" height="900" alt="Shows the Application Creation Fields for Secret Manifest " loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: The "Create an application" screen shows the test and dev labels. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Installing the secured cluster&lt;/h2&gt; &lt;p&gt;Each cluster runs an application. For simplicity, in this section you'll create two identical applications, one for the &lt;code&gt;dev&lt;/code&gt; cluster and one for the &lt;code&gt;test&lt;/code&gt; cluster. (Another approach would be to create one application and use templates to get the value for the cluster from the secrets, instead of hardcoding the value.)&lt;/p&gt; &lt;p&gt;For the simple approach we'll use here, go through these steps on each cluster:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Navigate to &lt;strong&gt;Applications→Create Application→Subscription&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Enter the name of the application and the namespace where you want to install it. For the &lt;code&gt;dev&lt;/code&gt; cluster, choose &lt;a href="https://github.com/sahilsethi12/acspolicy-gitops-acm/tree/main/secureclusterdeploypolicydevcluster"&gt;secureclusterdeploypolicy_devcluster&lt;/a&gt; as the Git path, and for the &lt;code&gt;test&lt;/code&gt; cluster, choose &lt;a href="https://github.com/sahilsethi12/acspolicy-gitops-acm/tree/main/secureclusterdeploypolicytestcluster"&gt;secureclusterdeploypolicy_testcluster&lt;/a&gt; as the Git path.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Figure 7 shows the values to enter.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/securedcluster.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/securedcluster.png?itok=vM7xKE9p" width="1440" height="900" alt="Shows the Application Creation Fields for Secured Cluster Installation" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: The "Create an application" screen shows options for the dev cluster. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Once everything is deployed, you can see the status of your clusters, along with some metrics, in the central server (Figure 8).&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Central.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Central.png?itok=T1E_yCKc" width="1440" height="900" alt="Shows the RHACS Central UI" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: The metrics dashboard shows the status and activites on your clusters. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has shown how configuration files and CI/CD pipelines can be used to manage security policies. The general principles can apply to other processes that you need to automate with GitOps.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/11/deploy-operator-gitops-using-advanced-cluster-management" title="Deploy an Operator via GitOps using Advanced Cluster Management"&gt;Deploy an Operator via GitOps using Advanced Cluster Management&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Sahil Sethi</dc:creator><dc:date>2022-07-11T07:00:00Z</dc:date></entry><entry><title type="html">How to export and import Realms in Keycloak</title><link rel="alternate" href="http://www.mastertheboss.com/keycloak/how-to-export-and-import-realms-in-keycloak/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/keycloak/how-to-export-and-import-realms-in-keycloak/</id><updated>2022-07-10T09:30:00Z</updated><content type="html">This article discusses about Importing and Exporting Keycloak Realms using the latest product distribution which runs on a Quarkus runtime. Realm Set up Firstly, if you are new to Keycloak and Quarkus, we recommend checking this article which covers the basics: Getting started with Keycloak powered by Quarkus A Keycloak Realm is a space where ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Explaining Drools with TrustyAI</title><link rel="alternate" href="https://blog.kie.org/2022/07/explaining-drools-trustyai.html" /><author><name>Rob Geada</name></author><id>https://blog.kie.org/2022/07/explaining-drools-trustyai.html</id><updated>2022-07-08T13:55:30Z</updated><content type="html">INTRODUCTION TO TRUSTYAI AND DROOLS Explainability is a crucial aspect in modern AI and decision services work; recent laws entitle any person subject to automated decisions to explanations of the intuition behind said decisions. Moreover, people are more likely to trust the decisions of explained models compared to unexplained models (Kim et al., 2022). Furthermore, explainability is very useful in introspecting models during the engineering process, to validate that the model is working according to design specifications and making ethical, legal, and fair decisions. However, providing intuitive explanations of a model’s workings can be difficult for large, complex models, especially so for "blackbox" models like deep neural networks or random forests. To address this issue, TrustyAI provides a suite of explainability algorithms such as , , and to explain any blackbox model. Meanwhile, business rules engines like Drools provide a powerful toolset to define rules, individual pieces of decision logic that outline some larger process. Drools in this regard has immense power and flexibility to produce complex and nuanced decision processes. While each individual rule should be fairly interpretable, in that the majority tend to follow if-then-else style logic, the composition of many rules en masse can make the entire ruleset quite hard to parse, especially for those that didn’t have a role in building the ruleset. In these circumstances, providing an explanation of a ruleset’s behavior is an intensive task, involving reading all the individual rules and attempting to manually understand how they interlink. Rulesets can be dynamic which makes this even harder, in that the particular rationale for one decision may involve ruleflows that are completely irrelevant or entirely unused for other decisions. A natural conclusion therefore is to try and explain Drools with TrustyAI, since TrustyAI’s algorithms can explain complex blackbox models and should therefore be well suited to explaining complex rulesets. MAPPING A RULESET INTO A MODEL The model paradigm; given some fixed input, produce some fixed output The term "model" in an explainability algorithm context refers to some function f that, when given some input x, produces some output y, i.e, something in the form y = f(x). What f, x, and y actually consist of is dependent on the particular usecase. For example, in a credit card application process, the input x could be a potential applicant’s credit details, the model f be some mechanism that evaluates the applicant, and the output y a binary accept/reject. An explainability algorithm would try to identify and explain how the specific facets or "features" within the input x affected the output y. However, the concepts of input, model, and output can be poorly defined within a rules engine like Drools. This is because Drools rulesets in implementation are simply a collection of Java classes, where the rules define how these classes interact and evolve throughout the execution of a ruleset. Rulesets can therefore take the form of basically any arbitrary Java program, in which case, how should input and output be defined in such a way that is most applicable to the majority of existing rulesets? Furthermore, can these defined inputs and outputs be automatically identified and extracted from existing rulesets without requiring any redesign of the ruleset, such that rulesets can be explained "out-of-the-box"? To satisfy these criteria, I’ve created the following definitions of input and output within Drools: INPUTS To initialize a rule evaluation, a series of objects are inserted into Drools. These inserted objects are logical choice for the inputs, specifically, any settable attribute within these objects or their recursive nested objects is defined as the model input. These attributes need to be settable due to the way that most explainability algorithms operate in a modify-and-observe fashion; they modify characteristics of the input and observe their effects on the output to understand the model’s internal workings. Therefore, the explanability algorithms need to be able to modify the inputs, meaning anything used as an input needs to be settable. OUTPUTS While the object insertion allows for a fairly simple definition of input, outputs are more amorphous as Drools does not specifically return anything from a ruleset evaluation. Instead, the evaluation of a ruleset modifies or deletes the inserted objects, or even creates new ones. These are the consequences of a ruleset evaluation, and are the most plausible candidates as the outputs of the system. Specifically, any of the following are considered a possible output: * Any gettable attribute of any object or recursive nested object that was created, modified, or deleted during ruleset evaluation * Any object that was created or deleted during ruleset evaluation The motivation to restrict attributes to be gettable is fairly obvious; in order to actually extract a created, modified, or deleted attribute it has to be retrievable in the first place. IMPLEMENTING THESE DEFINITIONS INPUT TRACKER Automatically identifying potential inputs is fairly straightforward; the user needs to pass a Supplier of all the objects that need to be inserted into Drools before ruleset evaluation. This initial input marks the initial values of all extracted features, and it is these values that will be explained by the TrustyAI algorithms. After the Object Supplier is created, all attributes of these objects are parsed recursively (such as to identify the settable attributes of all nested objects) for settable attributes. All found settable attributes are identified as candidate input features. This list can be narrowed down by a set of user-configurable filters that include or exclude specific rules, field names, or objects from consideration. Then a mapping between a TrustyAI PredictionInput and the attribute setters is generated, such that the set of objects-to-be-inserted can be generated, the relevant attributes set to desired values as per the PredictionInput (i.e., the particular values desired by the explainability algorithm as it tweaks the original input), and then passed into Drools. OUTPUT TRACKER Again, identifying potential outputs proves slightly harder. Since the goal is to track the creation, modification, or deletion of objects or gettable attributes during ruleset evaluation, a RuleFireListener is placed into the Drools engine. This tracks whenever any rule activates within the ruleset evaluation, and allows for the insertion of hooks before and after the activation. This functionality is exploited to monitor for potential outputs: before a rule fires, all objects and attribute values in the Drools engine are tracked. If any of these items have not been previously seen during evaluation, they are marked as novel items and thus potential outputs. After the rule fires, all objects and attribute values within the engine are again recorded. Any differences (in either item presence/absence or attribute value) between the before and after sets are marked as further potential outputs. After the full ruleset evaluation is complete, this process will have created a set of all objects and attributes that meet our output criteria. Again, this set can be narrowed down by a set of user-configurable filters that include or exclude specific rules, field names, or objects from consideration. Practically, this requires a single evaluation of the ruleset ahead of the actual explanation work, to track the various consequences of the specific initial input passed into Drools. This does limit the available output candidates to just those that were identified during this initial input. Novel outputs (i.e., consequences unseen during the evaluation of the initial input) are not valid output candidates. However, the system is robust to the absence of desired outputs, (that is, an output that was recorded during the evaluation of the initial input, but does not necessarily appear for other inputs) and as such a workaround to the novel output issue is to find an input that produces the desired output, and use its absence as the tracked output signal. USAGE With the input trackers and output trackers, we now have a schema by which to automatically input novel feature values into the rule engine and then extract our desired outputs. This lets a Drools ruleset evaluation be viewed as our model, which in turn lets Drools be explained via the TrustyAI explainers. In general, the workflow involved in producing the explanations looks as follows: 1. Define a Supplier&lt;List&lt;Object&gt;&gt;, a function that produces an initial set of objects to be inserted into Drools, thus defining the initial feature values. 2. Define a DroolsWrapper by specifying a Drools rule set and the Object Supplier. 3. Identify the available features within the supplied objects. 1. Narrow these features down by specifying filters, if desired. 2. If a counterfactual explanation is desired: * Specify feature boundaries to constrain the possible values of these features. 3. If a SHAP explanation is desired: * Specify background feature values. 4. Identify the available outputs. 1. Narrow these outputs down by specifying filters, if desired. 2. Choose a set of specific outputs to be marked as model outputs during explanation. 5. Wrap the DroolsWrapper into a TrustyAI PredictionProvider. 6. Use the PredictionProvider within any TrustyAI explanation algorithm, just like any other PredictionProvider model. EXPLAINING DROOLS WITH TRUSTYAI: EXAMPLES The Shipping Cost Calculation Ruleset We’ll use the example from Nicolas Héron’s gitbook, . In this ruleset, an Order is created consisting of a variety of products as well as a Trip which details the shipping route and modalities that the order must undergo. The evaluation of the ruleset then computes the various associated costs with the order shipment, like the tax, handling, and transportation costs. SETUP First, let’s define the Object Supplier: Supplier&lt;List&lt;Object&gt;&gt; objectSupplier = () -&gt; { // define Trip City cityOfShangai = new City(City.ShangaiCityName); City cityOfRotterdam = new City(City.RotterdamCityName); City cityOfTournai = new City(City.TournaiCityName); City cityOfLille = new City(City.LilleCityName); Step step1 = new Step(cityOfShangai, cityOfRotterdam, 22000, Step.Ship_TransportType); Step step2 = new Step(cityOfRotterdam, cityOfTournai, 300, Step.train_TransportType); Step step3 = new Step(cityOfTournai, cityOfLille, 20, Step.truck_TransportType); Trip trip = new Trip("trip1"); trip.getSteps().add(step1); trip.getSteps().add(step2); trip.getSteps().add(step3); // define Order Order order = new Order("toExplain"); Product drillProduct = new Product("Drill", 0.2, 0.4, 0.3, 2, Product.transportType_pallet); Product screwDriverProduct = new Product("Screwdriver", 0.03, 0.02, 0.2, 0.2, Product.transportType_pallet); Product sandProduct = new Product("Sand", 0.0, 0.0, 0.0, 0.0, Product.transportType_bulkt); Product gravelProduct = new Product("Gravel", 0.0, 0.0, 0.0, 0.0, Product.transportType_bulkt); Product furnitureProduct = new Product("Furniture", 0.0, 0.0, 0.0, 0.0, Product.transportType_individual); order.getOrderLines().add(new OrderLine(1000, drillProduct)); order.getOrderLines().add(new OrderLine(35000.0, sandProduct)); order.getOrderLines().add(new OrderLine(14000.0, gravelProduct)); order.getOrderLines().add(new OrderLine(500, furnitureProduct)); // combine Trip and Order into CostCalculationRequest CostCalculationRequest request = new CostCalculationRequest(); request.setTrip(trip); request.setOrder(order); return List.of(request) } While that was a little clunky, it is an implicit necessity of this specific ruleset; the only difference required by the TrustyAI-Drools integration is popping all of that inside the Supplier () -&gt; {etc} lambda. Next, we can initialize the DroolsWrapper and investigate possible features. // initialize the wrapper DroolsWrapper droolsWrapper = new DroolsWrapper(kieContainer,"CostRulesKS", objectSupplier, "P1"); FEATURE SELECTION With a DroolsWrapper created, we can investigate possible features: // setup Feature extraction droolsWrapper.displayFeatureCandidates(); This produces a massive list of possible features, a sample of which are shown below: === FEATURE CANDIDATES ====================================================== Feature | Value | Type | Domain ----------------------------------------------------------------------------- trip.steps[0].transportType | 1 | number | Empty trip.steps[2].stepStart.name | Tournai | categorical | Empty order.orderLines[0].product.height | 0.2 | number | Empty order.orderLines[1].product.depth | 0.0 | number | Empty order.orderLines[2].product.transportType | 3 | number | Empty order.orderLines[3].numberItems | 500 | number | Empty order.orderLines[1].product.width | 0.0 | number | Empty order.orderLines[1].product.name | Sand | categorical | Empty totalCost | 0.0 | number | Empty order.orderLines[3].product.transportType | 2 | number | Empty totalHandlingCost | 0.0 | number | Empty ... ... ... ============================================================================= The interesting choices among these for possible features are the variables, things that we would have the power to change. In the case of our shipment, it’s the shipping modality and product quantities, which we can isolate by setting up the following feature filters: // add filters via regex droolsWrapper.setFeatureExtractorFilters( List.of( "(orderLines\\[\\d+\\].weight)", "(orderLines\\[\\d+\\].numberItems)", "(trip.steps\\[\\d+\\].transportType)" ) ); // display candidates droolsWrapper.displayFeatureCandidates(); === FEATURE CANDIDATES ===================================== Feature | Value | Type | Domain ------------------------------------------------------------ order.orderLines[0].numberItems | 1000 | number | Empty trip.steps[0].transportType | 1 | number | Empty order.orderLines[3].numberItems | 500 | number | Empty order.orderLines[1].weight | 35000.0 | number | Empty order.orderLines[2].weight | 14000.0 | number | Empty trip.steps[1].transportType | 2 | number | Empty trip.steps[2].transportType | 3 | number | Empty ============================================================ These seem like good choices for the input, but one thing we notice here is that the trip.steps[*].transportType was automatically categorized as a numeric feature by the DroolsWrapper, likely due to however the transportType attribute is handled in the ruleset. This should really be a categorical feature, as there are only three possible values (1=Ship, 2=Train, 3=Truck). We’ll override the automatic type inference, as well as specifiy some feature domains (valid value ranges) for each of our features: // set feature type overrides, anything matching this regex will be categorical HashMap&lt;String, Type&gt; featureTypeOverrides = new HashMap&lt;&gt;(); featureTypeOverrides.put("trip.steps\\[\\d+\\].transportType", Type.CATEGORICAL); droolsWrapper.setFeatureTypeOverrides(featureTypeOverrides); // set feature domains for (Feature f: droolsWrapper.featureExtractor(objectSupplier.get()).keySet()) { if (f.getName().contains("transportType")){ // transport type can be truck, train, ship FeatureDomain&lt;Object&gt; fd = ObjectFeatureDomain.create(List.of(Step.truck_TransportType, Step.train_TransportType, Step.Ship_TransportType)); droolsWrapper.addFeatureDomain(f.getName(), fd); } else { // let numeric features range from 0 to original value FeatureDomain nfd = NumericalFeatureDomain.create(0., ((Number) f.getValue().getUnderlyingObject()).doubleValue()); droolsWrapper.addFeatureDomain(f.getName(), nfd); } } droolsWrapper.displayFeatureCandidates(); === FEATURE CANDIDATES ================================================ Feature | Value | Type | Domain ----------------------------------------------------------------------- trip.steps[0].transportType | 1 | categorical | [1, 2, 3] order.orderLines[3].numberItems | 500 | number | 0.0-&gt;500.0 order.orderLines[2].weight | 14000.0 | number | 0.0-&gt;14000.0 trip.steps[2].transportType | 3 | categorical | [1, 2, 3] order.orderLines[1].weight | 35000.0 | number | 0.0-&gt;35000.0 trip.steps[1].transportType | 2 | categorical | [1, 2, 3] order.orderLines[0].numberItems | 1000 | number | 0.0-&gt;1000.0 ======================================================================= Our features seem correctly configured, so let’s move onto output selection. OUTPUT SELECTION We’ll immediately apply some output filters to to remove irrelevant rules, objects, and attributes and focus on the interesting output candidates: // exclude the following objects droolsWrapper.setExcludedOutputObjects(List.of( "pallets", "LeftToDistribute", "cost.Product", "cost.OrderLine", "java.lang.Double", "costElements", "Pallet", "City", "Step", "org.drools.core.reteoo.InitialFactImpl", "java.util.ArrayList")); // exclude the following field names droolsWrapper.setExcludedOutputFields(List.of("pallets", "order", "trip", "step", "distance", "transportType", "city", "Step")); // only look at consequences of the following rules droolsWrapper.setIncludedOutputRules(List.of("CalculateTotal")); droolsWrapper.generateOutputCandidates(true); Which produces the following results: === OUTPUT CANDIDATES ============================================================ Index | Rule | Field Name | Final Value ---------------------------------------------------------------------------------- 0 | CalculateTotal | totalHandlingCost | 5004.0 1 | CalculateTotal | rulebases.cost.CostCalculationRequest_1 | Created 2 | CalculateTotal | totalTransportCost | 2499790.0 3 | CalculateTotal | totalCost | 2505075.8 4 | CalculateTotal | numPallets | 547 5 | CalculateTotal | totalTaxCost | 281.8 ================================================================================== A SIMPLE SHAP EXPLANATION From these candidate outputs, let’s investigate how each of our items affected the Total Cost and Tax Cost of the shipment using SHAP. First, we’ll set these two as our desired outputs, using their indeces shown in table above: // select the 2nd and 5th options from the generated candidates droolsWrapper.selectOutputIndicesFromCandidates(List.of(2,5)); Next, we need to specify a background input to compare against in order to use SHAP. In our case, we’ll use a shipment containing 0 items/kilos of each item, all shipped by Truck as the comparison baseline: List&lt;Feature&gt; backgroundFeatures = new ArrayList&lt;&gt;(); for (int j = 0; j &lt; samplePI.getFeatures().size(); j++) { Feature f = samplePI.getFeatures().get(j); if (f.getName().contains("transportType")) { backgroundFeatures.add(FeatureFactory.copyOf(f, new Value(Step.truck_TransportType))); } else { backgroundFeatures.add(FeatureFactory.copyOf(f, new Value(0.))); } } List&lt;PredictionInput&gt; background = List.of(new PredictionInput(backgroundFeatures)); We can then run SHAP: Explainers.runSHAP(droolsWrapper, background) ----------------- OUTPUT CALCULATETOTAL: TOTALTRANSPORTCOST ----------------- Feature : SHAP Value FNull : 0.000 order.orderLines[1].weight = 35000.0 : 336125.000 +/- 1587245.191 order.orderLines[2].weight = 14000.0 : 134450.000 +/- 1587245.191 trip.steps[2].transportType = 3 : 0.000 +/- 0.000 order.orderLines[3].numberItems = 500 : 6722500.000 +/- 1587245.191 order.orderLines[0].numberItems = 1000 : 161340.000 +/- 1587245.191 trip.steps[1].transportType = 2 : -41025.000 +/- 1587245.191 trip.steps[0].transportType = 1 : -4813600.000 +/- 3549188.144 --------------------------------------------------------------------------- Prediction : 2499790.000 ----------------- OUTPUT CALCULATETOTAL TOTALTAXCOST ----------------- Feature : SHAP Value FNull : 33.000 order.orderLines[1].weight = 35000.0 : 98.000 +/- 0.000 order.orderLines[2].weight = 14000.0 : 98.000 +/- 0.000 trip.steps[2].transportType = 3 : 0.000 +/- 0.000 order.orderLines[3].numberItems = 500 : 32.000 +/- 0.000 order.orderLines[0].numberItems = 1000 : 20.800 +/- 0.000 trip.steps[1].transportType = 2 : 0.000 +/- 0.000 trip.steps[0].transportType = 1 : -0.000 +/- 0.000 --------------------------------------------------------------------- Prediction : 281.800 From these explanations, we can see a few interesting things. Namely, shipping by truck is the most expensive option as compared to shipping by train or ship; for example, the trip.steps[0].transportType= 1 : -4813600.000 shows that shipping the first leg of the journey via ship (transportType=1 in this particular example) saved $4,813,600 from the total cost as compared to shipping via truck. Additionally, we can see that the choice of shipping method has no effect on the tax cost: Feature : SHAP Value trip.steps[1].transportType = 2 : 0.000 +/- 0.000 trip.steps[0].transportType = 1 : -0.000 +/- 0.000 trip.steps[2].transportType = 3 : 0.000 +/- 0.000 A SIMPLE COUNTERFACTUAL EXPLANATION Another cool thing we can do is produce a counterfactual explanation of the model. Say for example we only had a shipping budget of $2,000,000, we can use the counterfactual explainer to find the closest shipment to our original one that meets our constraint. To do this, we need to specify our Counterfactual Goal: List&lt;Output&gt; goal = List.of( new Output( "rulebases.cost.CostCalculationRequest.totalCost", Type.NUMBER, new Value(2_000_000), // this is where we set our goal to 2 million 0.0d) ); Then we can run the counterfactual explainer: Explainers.runCounterfactualSearch( droolsWrapper, goal, .01, // want to get within 1% of goal 60L // allow 60 seconds of search time ); which, after a minute, outputs: === COUNTERFACTUAL INPUTS ====================================== Feature | Original Value → Found Value ---------------------------------------------------------------- order.orderLines[1].weight | 35000.0 → 35000.0 order.orderLines[2].weight | 14000.0 → 14000.0 order.orderLines[3].numberItems | 500 → 386 trip.steps[2].transportType | 3 → 3 trip.steps[1].transportType | 2 → 2 trip.steps[0].transportType | 1 → 1 order.orderLines[0].numberItems | 1000 → 1000 ================================================================ === COUNTERFACTUAL OUTPUT ====================================== Output | Original Value → Found Value ---------------------------------------------------------------- CalculateTotal: totalCost | 2505075.8 → 1983049.8 ================================================================ Meets Criteria? true The solution the counterfactual explainer has found makes just one change to the order (reducing order.orderLines[3].numberItems from 500 to 386) which results in a new totalCost of 1.98 million, which meets the criteria we set out originally. A MORE COMPLEX COUNTERFACTUAL We can try for even more complex outcomes too, for example, what if we wanted to keep our total shipped pallet count as close to unchanged as possible, while reducing our tax cost to $200? To do this, we can set our output targets in the DroolsWrapper and create a new Counterfactual search goal. Here, we’ll set the numPallets goal to 547 (our original pallet count) and the totalTaxCost goal to 200. droolsWrapper.selectOutputIndicesFromCandidates(List.of(4,5)); List&lt;Output&gt; goal = List.of( new Output("rulebases.cost.CostCalculationRequest.numPallets", Type.NUMBER, new Value(540), 0.0d), new Output("rulebases.cost.CostCalculationRequest.totalTaxCost", Type.NUMBER, new Value(200), 0.0d) ); Explainers.runCounterfactualSearch(droolsWrapper, goal, .005, //aim for within 0.5% of goals 300L //allow for 5 minutes of search time ); The counterfactual explainer will now try and find an input configuration that meets both criteria, and does so: === COUNTERFACTUAL INPUTS ============================================= Feature | Original Value → Found Value ----------------------------------------------------------------------- order.orderLines[0].numberItems | 1000 → 1000 trip.steps[0].transportType | 1 → 1 order.orderLines[3].numberItems | 500 → 498 trip.steps[1].transportType | 2 → 2 order.orderLines[1].weight | 35000.0 → 35000.0 trip.steps[2].transportType | 3 → 3 order.orderLines[2].weight | 14000.0 → 12841.676 ======================================================================= === COUNTERFACTUAL OUTPUT ============================================= Output | Original Value -&gt; Found Value ----------------------------------------------------------------------- numPallets | 547.0 -&gt; 545.0 totalTaxCost | 281.80 -&gt; 200.717 ======================================================================= Meets Criteria? true The counterfactual explainer has found a solution that removes just 2 pallets from the shipment, while reducing the tax cost from $281.80 down to $200.72. GET THE CODE The full code for the demo above can be seen . The entire TrustyAI-Drools repo is at . CONCLUSION In this blogpost we’ve taken a look at how to integrate TrustyAI’s explainers into Drools, allowing for the explanation of Drools ruleset evaluations via TrustyAI’s explanability algorithms. We’ve also taken a look at an example use-case, exploring how the explainers can give us insight about the functionality of rulesets as well as provide a new set of interesting features to Drools itself. The post appeared first on .</content><dc:creator>Rob Geada</dc:creator></entry><entry><title type="html">Kogito 1.24.0 released!</title><link rel="alternate" href="https://blog.kie.org/2022/07/kogito-1-24-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2022/07/kogito-1-24-0-released.html</id><updated>2022-07-08T06:38:35Z</updated><content type="html">We are glad to announce that the Kogito 1.24.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Persistence support for Process versions * Allow users to set up the configuration key for open API properties by setting  kogito.sw.operationIdStrategy. If none of the predefined approaches are suitable, users can define their own config key by using UriDefinitions extension. Possible values for the new property are: * FILE_NAME:  use the last element of the spec uri path. * FULL_URI:  use the full uri path to generate the key. * FUNCTION_NAME:  use the workflow id and the function name that references the spec uri * SPEC_TITLE: use specification title * Added the new kogito-quarkus-serverless-workflow-devui for Serverless Workflow testing * PostgreSQL persistence addon to support correlation BREAKING CHANGES * The swf-quarkus-extension module had its artifactId changed to kogito-quarkus-serverless-workflow-devui For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.20.0 artifacts are available at the . A detailed changelog for 1.24.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title type="html">Saying goodbye to Red Hat</title><link rel="alternate" href="http://www.schabell.org/2022/07/saying-goodbye-to-red-hat.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2022/07/saying-goodbye-to-red-hat.html</id><updated>2022-07-08T05:00:00Z</updated><content type="html">My tenure: 2009-2022 The time has come... the end of my tenure at Red Hat after +13 years.   How about a short summary of some of the highlights while at Red Hat before I move on to a new adventure?  I'll try to capture the big milestones, but there are just so many that I'm sure to miss a few.  It's a moment of reflection on more than a decade spent in the world of enterprise open source technologies and riding a wave that was Red Hat in the prime of its evolution in the industry. I have been involved in open source since my introduction late in life to programming, operating systems, and Linux in 1996. Before joining Red Hat I was involved in the community around a business process management project called . We were using this heavily in the financial institution I was working at and I became active as you do when you submerge yourself into an open source technology. In those days it was still jBPM 3 and we were having the time of our lives learning to model and implement processes. We even went crazy with a , to just give you an example of what we thought was possible back in those days.  It all led to Red Hat knocking on my door in 2009 to join the Netherlands team as their first middleware (JBoss) solution architect.  At the time I joined I remember the SA team was about 15 associates across Europe, Red Hat had around 2000 associates world wide, and the stock price was between $22-$24 (later sold to IBM for $190).  It was incredible fun and there was a very active start up feel to what we were doing. My experiences with JBoss technologies and specifically the Business Rules Management System (BRMS) led to me spending more time across Europe helping other sales teams than in my own region. I was also still active in the community around those products, meaning Drools, jBPM, and other JBoss projects upstream.  This role I held for approximately three years before the product orgnanization came knocking. My contacts in the product teams led to my being asked to join the Middleware Business Unit as one of their first MW Technical Marketing Managers or TMM. At this time I think we had like 4-5 products so one person could manage the role. I ended up spending around four years in this role and we grew not only our MW product portfolio, but the team eventually grew out to five TMM's before I moved on.  We worked on products like JBoss SOA-P, Switchyard, JBoss Virtualization, JBoss BPM Suite, the first versions of OpenShift (remember gears?), and so much more. This role required enablement sessions being delivered to the field and we visited all our regional offices; APAC (Singapore, Tokyo, Beijing), EMEA (Munich, London, Stockholm, Madrid, Amsterdam, Brussels, Rome, Warsaw, etc), NA (Boston, Dallas, Raleigh, Mountain View, Portland, St. Louis, Tampa, etc), and LATAM (Mexico City, San Paulo, etc).  Right at the end of that role I got the chance to move to the US, out on an island off the coast of North Carolina for a year. Quite the adventure and my family loved every minute of our island time. The next role to come along was in a brand new business unit, called the Integrated Solutions Business Unit, setup to try and pull together the first multi-product products for our field to sell. I spent the next two years working with some serious rock stars as we pulled together Red Hat Cloud Infrastructure and Red Hat Cloud Suite products. I was the TMM for both of these, and due to our PMM leaving before the launch of the Red Hat Cloud Suite I picked up the PMM work for the product launch.  Part of this period also involved the setting up of the internal Red Hat TMM Practice, where all existing and new TMM's could find a baseline on how to be effective in their roles. We also worked with Red Hat HR and developed a well defined ladder for the creation of the official TMM role in Red Hat.  My final role was to help setup a new concept, that of the Portfolio Architecture team. We have spent the last four years defining, researching, receiving field feedback, and creating 25 published architectures that you can explore on the Red Hat Portfolio Architecture Center. This team started with just three of us, but has since grown out to seven and I had the honour of being the technical director of this developing product.  All along this journey through Red Hat I've spent many hours mentoring all manner of TMM's (and other associates) as I am a true believer that we all have something to share to make others better. I never suspected how long this would last when I walked into that first interview and they asked me to draw a SOA architecture on a whiteboard. I look around now 13 yrs, 106 days later and I'm still engaged at Red Hat.  Now this brings me to the part were we say goodbye and thank you all for the fun we've had together. There are so many colleagues that have become both friends and family, so it's not even possible to name them (but you know who you are). My last day is next week Friday, 15 July. What's next you ask?  Stay tuned for more on that after I take a break between the old and the new. </content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">Refactoring the Drools Compiler</title><link rel="alternate" href="https://blog.kie.org/2022/07/refactoring-the-drools-compiler.html" /><author><name>Edoardo Vacchi</name></author><id>https://blog.kie.org/2022/07/refactoring-the-drools-compiler.html</id><updated>2022-07-07T14:00:00Z</updated><content type="html">In the past few weeks we have been working hard on redesigning the architecture of Drools the rules engine and the rest of our ecosystem of runtime engines. In this blog post I want to focus a bit on the refactoring of the KnowledgeBuilder, a core component of the build infrastructure of the v6-v7 APIs. For the ongoing work on Drools 8, we are rethinking the entire design of this and other components. On the latest stable version of the 7 series, that contains logic for processing resources of different types (such as DRL, DMN, BPMN, PMML, XLS etc…) On the main branch, , where most of the fat is really public methods that we kept for backwards compatibility, that are now delegating to new self-contained classes. The main culprit with the KnowledgeBuilderImpl was that it was both the class holding the logic for building assets, and both a sort of "context" object that was passed around to collect pieces of information. The main goals of the refactoring were 1. Refactoring most of the state inside the KnowledgeBuilderImpl into smaller objects with well-defined boundaries 2. Moving the building logic related to the DRL family (plain DRL, XLS, DSLs etc.) to a series smaller, composable 3. Ensuring that each CompilationPhase never referred directly the KnowledgeBuilderImpl The same work involved the CompositeKnowledgeBuilderImpl (which decorates KnowledgeBuilderImpl) and for the ModelBuilderImpl (which subclasses the KnowledgeBuilderImpl). As you can imagine the work was a bit long and iterative, but the good news is that it is now possible to put the CompositePhases in sequence, instantiating them without requiring the entire KnowledgeBuilder, but just its constituent. The KnowledgeBuilderImpl itself now implements by delegating to self-contained objects (e.g. , , ). The phases always refer to such interfaces, e.g., a only refers to a . The result is that now it is possible to put in sequence such phases to produce a: List&lt;CompilationPhase&gt; phases = asList( new ImportCompilationPhase(packageRegistry, packageDescr), new TypeDeclarationAnnotationNormalizer(annotationNormalizer, packageDescr), new EntryPointDeclarationCompilationPhase(packageRegistry, packageDescr), new AccumulateFunctionCompilationPhase(packageRegistry, packageDescr), new TypeDeclarationCompilationPhase(packageDescr, typeBuilder, packageRegistry, null), new WindowDeclarationCompilationPhase(packageRegistry, packageDescr, typeDeclarationContext), new FunctionCompilationPhase(packageRegistry, packageDescr, configuration), new ImmutableGlobalCompilationPhase(packageRegistry, packageDescr, globalVariableContext), new RuleAnnotationNormalizer(annotationNormalizer, packageDescr), new RuleValidator(packageRegistry, packageDescr, configuration), new ImmutableFunctionCompiler(packageRegistry, packageDescr, rootClassLoader), new ImmutableRuleCompilationPhase(packageRegistry, packageDescr, parallelRulesBuildThreshold, attributesForPackage, resource, typeDeclarationContext), new ConsequenceCompilationPhase(packageRegistryManager) ); The same is true both for the traditional in-memory compiler, and . This huge refactoring makes it possible to reuse most of the logic in the traditional compilation flow in a new compiler architecture that is currently being worked on. Stay tuned for more details! The post appeared first on .</content><dc:creator>Edoardo Vacchi</dc:creator></entry><entry><title>Add an Infinispan cache to your ASP.NET application</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/07/add-infinispan-cache-your-aspnet-application" /><author><name>Vittorio Rigamonti</name></author><id>290f5459-e17a-43b6-ae71-20d8acf9484e</id><updated>2022-07-07T07:00:00Z</updated><published>2022-07-07T07:00:00Z</published><summary type="html">&lt;p&gt;The open source &lt;a&gt;Infinispan&lt;/a&gt; data store is popular for in-memory operations. A &lt;a href="https://developers.redhat.com/topics/dotnet"&gt;.NET Core&lt;/a&gt; application can now easily integrate Infinispan as a caching service or session provider. This article provides basic information on how to do that in &lt;a href="https://developers.redhat.com/topics/c"&gt;C#&lt;/a&gt; on &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;What you need:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;.NET 6.0+ installed on your system&lt;/li&gt; &lt;li&gt;Access to get packages from &lt;a href="https://www.nuget.org"&gt;NuGet&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Access to an Infinispan server&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;On the Infinispan server, create a cache named &lt;code&gt;default&lt;/code&gt; listening on the loopback host and port 127.0.0.1:11222, without authentication.&lt;/p&gt; &lt;h2&gt;Create the application&lt;/h2&gt; &lt;p&gt;You are going to work on an &lt;a href="https://docs.microsoft.com/en-us/aspnet/core/?view=aspnetcore-6.0"&gt;ASP.NET Core&lt;/a&gt; application, so run the following command to generate a new application scaffold:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;dotnet new webapp -lang 'C#' -n Infinispan.Example.Caching -f net6.0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command creates a new folder with an empty but working ASP.NET Core application. Run the application as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cd Infinispan.Example.Caching dotnet run&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check the application's log message for its HTTP or HTTPS URL and enter it into your browser to see the application's display.&lt;/p&gt; &lt;h2&gt;Add Infinispan as a cache&lt;/h2&gt; &lt;p&gt;The application requires the Infinispan caching package, so add it as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;dotnet add package Infinispan.Hotrod.Caching --version 0.0.1-alpha3&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This package provides an IDistibutedCache implementation based on the Infinispan C# client, imported as a dependency.&lt;/p&gt; &lt;h2&gt;Set up Infinispan as a cache provider&lt;/h2&gt; &lt;p&gt;An ASP.NET Core application (6.0) provides all the services and pipeline setup in the &lt;code&gt;Program.cs&lt;/code&gt; file. To this file, you need to add the Infinispan client configuration in the service setup, as well as session management in the process pipeline. Set the cache entries to expire after 10 seconds of idle time. &lt;code&gt;Program.cs&lt;/code&gt; should now look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;using Infinispan.Hotrod.Core; using Infinispan.Hotrod.Caching.Distributed; var builder = WebApplication.CreateBuilder(args); // Add services to the container. // Infinispan default setup is used: // 127.0.0.1:11222 cacheName: default builder.Services.AddInfinispanCache(); builder.Services.AddSession(options =&gt; { options.IdleTimeout = TimeSpan.FromSeconds(10); options.Cookie.HttpOnly = true; options.Cookie.IsEssential = true; }); builder.Services.AddRazorPages(); var app = builder.Build(); // Configure the HTTP request pipeline. if (!app.Environment.IsDevelopment()) { app.UseExceptionHandler("/Error"); // The default HSTS value is 30 days. You may want to change this for production scenarios, see https://aka.ms/aspnetcore-hsts. app.UseHsts(); } app.UseHttpsRedirection(); app.UseStaticFiles(); app.UseRouting(); app.UseAuthorization(); app.UseSession(); app.MapRazorPages(); app.Run();&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Add business code&lt;/h2&gt; &lt;p&gt;Our business code is very simple: It presents some information (the user name, the age of the session, and the first access time) to the user as a result of a request. The data is fetched from the session cache if available there and computed by the program otherwise. This logic is implemented in the model file &lt;code&gt;Pages/Index.cshtml.cs&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;using Microsoft.AspNetCore.Mvc; using Microsoft.AspNetCore.Mvc.RazorPages; namespace Infinispan.Example.Caching.Pages { public class IndexModel : PageModel { private readonly ILogger&lt;IndexModel&gt; _logger; public IndexModel(ILogger&lt;IndexModel&gt; logger) { _logger = logger; } public const string SessionKeyName = "_Name"; public const string SessionKeyAge = "_Age"; public const string SessionKeyFirstAccess = "_FirstAccess"; public static Random RndSource = new Random(); public string DataSource { get; private set; } = ""; public void OnGet() { // Requires: using Microsoft.AspNetCore.Http; if (string.IsNullOrEmpty(HttpContext.Session.GetString(SessionKeyName))) { DataSource = "Computed"; HttpContext.Session.SetString(SessionKeyName, "Mickey"); HttpContext.Session.SetInt32(SessionKeyAge, RndSource.Next(100)); HttpContext.Session.SetString(SessionKeyFirstAccess, DateTime.Now.ToString()); return; } DataSource = "Cache"; } } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output is generated from a file named &lt;code&gt;Pages/Index.cshtml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-html"&gt;@page @using Infinispan.Example.Caching.Pages @using Microsoft.AspNetCore.Http @model IndexModel @{ ViewData["Title"] = "Home page"; } &lt;div class="text-center"&gt; &lt;h1 class="display-4"&gt;Welcome&lt;/h1&gt; &lt;h2&gt;The time on the server is @DateTime.Now&lt;/h2&gt; &lt;h2&gt;Name (@Model.DataSource): @HttpContext.Session.GetString(IndexModel.SessionKeyName)&lt;/h2&gt; &lt;h2&gt;Age (@Model.DataSource): @HttpContext.Session.GetInt32(IndexModel.SessionKeyAge)&lt;/h2&gt; &lt;h2&gt;First Access Date (@Model.DataSource): @HttpContext.Session.GetString(IndexModel.SessionKeyFirstAccess)&lt;/h2&gt; &lt;p&gt;Learn about &lt;a href="https://docs.microsoft.com/aspnet/core"&gt;building Web apps with ASP.NET Core&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Easy ASP.NET access to Infinispan&lt;/h2&gt; &lt;p&gt;Everything is now in place for the show. Run your application again and check the output in the browser. The &lt;code&gt;Age &lt;/code&gt;and the &lt;code&gt;First Access Date&lt;/code&gt; field are computed the first time and cached for 10 seconds. The session itself expires after 10 seconds of idle time, as configured in the services configuration. Therefore, the cached values are reused by the application if requests come in quick succession.&lt;/p&gt; &lt;p&gt;This article has demonstrated how easily you can integrate Infinispan as a distributed cache and session provider for ASP.Net Core applications.&lt;/p&gt; &lt;p&gt;Explore more .NET tutorials on Red Hat Developer:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/03/21/hello-podman-using-net"&gt;Hello Podman using .NET&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/02/22/debug-net-applications-running-local-containers-vs-code"&gt;Debug .NET applications running in local containers with VS Code&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/07/07/deploy-net-applications-red-hat-openshift-using-helm"&gt;Deploy .NET applications on Red Hat OpenShift using Helm&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/07/add-infinispan-cache-your-aspnet-application" title="Add an Infinispan cache to your ASP.NET application"&gt;Add an Infinispan cache to your ASP.NET application&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Vittorio Rigamonti</dc:creator><dc:date>2022-07-07T07:00:00Z</dc:date></entry><entry><title type="html">Vlog: Encrypting a Filesystem Realm</title><link rel="alternate" href="https://www.youtube.com/watch?v=1K92tit2uCk" /><author><name>Ashpan Raskar</name></author><id>https://www.youtube.com/watch?v=1K92tit2uCk</id><updated>2022-07-07T00:00:00Z</updated><dc:creator>Ashpan Raskar</dc:creator></entry><entry><title type="html">Creating Prometheus Dashboards using Dashbuilder</title><link rel="alternate" href="https://blog.kie.org/2022/07/creating-prometheus-dashboards-using-dashbuilder.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2022/07/creating-prometheus-dashboards-using-dashbuilder.html</id><updated>2022-07-06T15:27:00Z</updated><content type="html">is a client tool for building dashboards and can consume data from any JSON source including Prometheus! It runs without a server requirement and the dashboard definition uses . You can run YML in an or create a static website as described in the .  In this article we will describe how to create Dashbuilder dashboards for Prometheus and share some samples so you can get started creating your own dashboards. PROMETHEUS HTTP API AND PROMQL Prometheus can be accessed using HTTP  to execute PromQL queries and have results in JSON format. The result of HTTP calls can have different types: Instant vector, Range vector and Scalar. According to the result type then we need to transform to the format supported by Dashbuilder. Dashbuilder can consume any as a dataset or a more complex object to include metadata. To transform results we need to use a expression, which may seem complex at a first glance, but once it is built we do not have to modify it anymore. Here’s the simplest dashboard that uses an expression that can be reused for any Prometheus dashboard: properties: prometheusUrl: http://localhost:9090 query: 1 parse: &gt;- &gt;- $.data.( { "columns": result[0].( [ {"id" : "timestamp", "type": "number"}, {"id" : "value", "type": "number"}, $keys(metric).({"id" : $, "type": "label"}) ]; ), "values": ( resultType = "scalar" ? [result[0] * 1000, result[1]] : resultType = "matrix" ? result.( $metric := metric.*; values.[ $[0] * 1000, $[1], $metric ] ) : resultType = "vector" ? result.[ value[0] * 1000, value[1], metric.* ] ) } ) datasets: - uuid: prometheus expression: ${parse} url: ${prometheusUrl}/api/v1/query?query=${query} pages: - components: - settings: lookup: uuid: prometheus This specific expression can be modified if needed. Since it is set as an expression then you can reuse it for all datasets you are creating. VISUAL COMPONENTS To show data we have multiple that can be declared .  A common component to be used with Prometheus is timeseries. This component only requires 3 columns from the dataset:  1. series: a column with values that will be used as the series; 2. timestamp: the timestamp column in javascript supported formats (the expression used above handle this) 3. value: The value used in axis Y Here’s a timeseries for the query prometheus_http_requests_total[1h:10s] filtering only successful HTTP requests: - settings: component: timeseries refresh: interval: "2" timeseries: title: text: Successful Responses to Prometheus external: width: 100% height: 400px lookup: uuid: prometheus filter: - column: code function: EQUALS_TO args: - 200 group: - functions: - source: handler - source: timestamp - source: value We can show total values using components that allow us to show a specific value which can be the result of an aggregation operation, such as sum or average. Here’s for example 3 cards in a row summarizing http requests received by prometheus: columns: - span: "4" components: - settings: type: METRIC general: title: "All" visible: "true" chart: height: "90" columns: - id: value pattern: "#,000" lookup: uuid: http_requests group: - functions: - source: value function: SUM - span: "4" components: - settings: type: METRIC general: title: "Success" visible: "true" chart: height: "90" columns: - id: value pattern: "#,000" lookup: uuid: http_requests filter: - column: code function: EQUALS_TO args: - 200 group: - functions: - source: value function: SUM - span: "4" components: - settings: type: METRIC general: title: "Others" visible: "true" chart: height: "90" columns: - id: value pattern: "#,000" lookup: uuid: http_requests filter: - column: code function: NOT_EQUALS_TO args: - 200 group: - functions: - source: value function: SUM    Finally we can add a filter component to allow us to read metrics about a specific handler: - components: - html: &lt;h1&gt; Prometheus HTTP Requests&lt;/h1&gt; &lt;hr/&gt; - html: "Filter" properties: font-weight: bolder - properties: width: "150px" margin-bottom: 30px settings: type: SELECTOR refresh: interval: "${refreshInterval}" filter: enabled: "true" notification: "true" lookup: uuid: recent_http_requests group: - columnGroup: source: handler functions: - source: handler We can make the report real time by adding a refresh interval to each component and to avoid concurrent requests we can also add cache to our dataset. Adding all together we have the following report: Check the code for the dashboard above in . CONCLUSION Dashbuilder highlights that it can run without the requirement of a server installation and has a comprehensive YML guide, which makes it a great tool for creating dashboards for Prometheus.  For more examples check the ! The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry><entry><title>What qualifies for Red Hat Developer Subscription for Teams?</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/06/what-qualifies-red-hat-developer-subscription-teams" /><author><name>Josh Swanson, Brian Gollaher</name></author><id>d60e8bf3-bc55-40a1-9b6c-cb96c30b3df5</id><updated>2022-07-06T07:00:00Z</updated><published>2022-07-06T07:00:00Z</published><summary type="html">&lt;p&gt;Recently, Red Hat &lt;a href="https://developers.redhat.com/articles/2022/05/10/access-rhel-developer-teams-subscription"&gt;relaunched&lt;/a&gt; the Developer Subscription for Teams, enabling organizations &lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;already running other Red Hat technologies to&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; access &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; for their development activities without friction. In this article, we clarify what Red Hat defines as development activities and highlight some exciting use cases for the Developer Subscription for Teams.&lt;/p&gt; &lt;h2&gt;What activities are considered development use?&lt;/h2&gt; &lt;p&gt;Figure 1 calls out the delimitation between development and production activities according to &lt;a href="https://www.redhat.com/en/about/agreements"&gt;Appendix 1&lt;/a&gt; of the Red Hat license agreement. To begin breaking this down, let’s start at the far left end, where systems would leverage the Developer Subscription for Teams.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Development-Use-Cases_diagram.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Development-Use-Cases_diagram.png?itok=VAp0P1Op" width="1440" height="422" alt="Development activities grouped under RHEL Developer for Teams and production activities grouped under RHEL." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: How activities are categorized for development and production according to the Red Hat Enterprise Agreements.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;An example application&lt;/h2&gt; &lt;p&gt;Let’s consider a fairly simple yet mission-critical application: a tax calculator. When most businesses run transactions for goods and services, the appropriate taxes must be calculated and added to the total purchase price. Our example application consists of three parts:&lt;/p&gt; &lt;ul&gt; &lt;li aria-level="1"&gt;A database that stores the appropriate tax information and rates.&lt;/li&gt; &lt;li aria-level="1"&gt;A middle tier that retrieves tax information from the database and performs calculations.&lt;/li&gt; &lt;li aria-level="1"&gt;A front-end system running a RESTful API that’s used to interact with the application.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These three tiers come together to form our tax application. They will be the basis for considering how the Developer Subscription for Teams enables the rapid development, testing, and deployment of this application. Now we’ll walk through each phase, as illustrated in Figure 1.&lt;/p&gt; &lt;h2&gt;Software design and coding&lt;/h2&gt; &lt;p&gt;When developing, updating, or maintaining this tax application, there are often design phases meant to outline the intended end state: functionality, architecture, features, and components. Once the design is accepted, then the actual work of writing the code begins. The systems used for designing a new application or upgrading an existing one with new features, overhauling legacy code, squashing bugs, etc., would all be covered by the Developer Subscription for Teams.&lt;/p&gt; &lt;h2&gt;Building&lt;/h2&gt; &lt;p&gt;Once the code for our tax application is written, we might need to build the application, which means pulling in dependencies, setting appropriate values, and outputting them into a form that can easily be deployed. We might also need to build multiple deployment packages that can be deployed across multiple systems in multiple environments and compute spaces, such as a cloud provider or on-premise datacenter.&lt;/p&gt; &lt;p&gt;Since this action might require more computational power than what a standard workstation provides, it makes sense to leverage the capabilities of a hyperscaler. The systems we use to build the application would also qualify for the Developer Subscription for Teams.&lt;/p&gt; &lt;h2&gt;Unit testing&lt;/h2&gt; &lt;p&gt;Our three-tier tax application, composed of three parts (a database, a middle tier, and a front-end API), can be broken apart and tested individually, allowing for more thorough testing of the components and more targeted testing.&lt;/p&gt; &lt;p&gt;Our database can be loaded with the appropriate tax information and then put through a schema update test to ensure tax information can be updated without application downtime.&lt;/p&gt; &lt;p&gt;Second, our middle tier can be put through a load test to demonstrate expected response times and validate that multiple calculations can be run simultaneously without causing calculation collisions.&lt;/p&gt; &lt;p&gt;Finally, our front-end API’s role-based access control (RBAC) functionality can be tested to ensure only requests coming from the appropriate systems are accepted and sent to be processed by the application, and that the API returns the results in a data format that other applications can easily consume.&lt;/p&gt; &lt;p&gt;What’s unique about this specific step in the software development life cycle is that the number of systems needed to properly test and validate each unit of the application increases almost exponentially. A database system is necessary, and we also need a system to run some test transactions via a connection to the database while another system is running a schema update. Our middle-tier system needs multiple other systems connected and requesting taxes be calculated while being able to reference an external source of controlled data to ensure calculations are accurate. Finally, our API needs to be receiving requests from different sources, some of which are allowed via RBAC rules and some that are not, while being able to return consumable data to those authorized sources.&lt;/p&gt; &lt;h2&gt;System integration and integration testing&lt;/h2&gt; &lt;p&gt;At this step in the software development life cycle, we start to put the puzzle pieces of our application landscape together. Here, we’ll combine our example tax application with our existing point-of-sale application, our ledger application, our inventory tracking application, and so on. All these applications must work together successfully for our customers to be able to make a purchase, so testing and validation of the integration of these various applications is crucial.&lt;/p&gt; &lt;p&gt;First, we can spin up the other applications and ensure they’re communicating with each other; then, we can add in our example tax application. Once the appropriate API endpoints are specified, RBAC rules are written, and network communication is validated, we can test running real-world transactions through our system and look for any piece of the puzzle that isn’t working or is blocking a successful purchase. We can also use real-world data here to ensure our test environment completes the purchase as expected.&lt;/p&gt; &lt;h2&gt;Pre-prod testing&lt;/h2&gt; &lt;p&gt;Here, we develop, test, and practice the steps needed to safely introduce our new example tax application to our production environment in a space that closely resembles it. We’ll walk through identifying what outages are required and what impact that will have on our business, as well as clearly documenting any steps that need to be taken by teams to deploy our new tax application successfully. These steps can be tested multiple times for training and practice purposes in this environment without impacting production.&lt;/p&gt; &lt;p&gt;An environment such as this is often a scaled-down version of production that is constantly running, and has multiple teams that can access and leverage it for various activities. Again, the Developer Subscription for Teams covers this environment, removing the subscription barrier to entry and having an appropriate place to test the introduction of our tax application to production.&lt;/p&gt; &lt;h2&gt;Push to production&lt;/h2&gt; &lt;p&gt;Here’s where we cross the bridge from the Developer Subscription for Teams into “production” Red Hat Enterprise Linux subscriptions. Let’s say our organization uses a combination of continuous integration/continuous delivery (CI/CD) pipelines and &lt;a href="https://developers.redhat.com/articles/2022/05/26/whats-new-ansible-automation-platform-22"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; to deploy our applications in production from our code repository. The two are closely integrated to perform the steps we identified during testing in our pre-prod environment.&lt;/p&gt; &lt;p&gt;Our Red Hat Ansible Automation Platform subscriptions would cover the automation piece of the push-to-production story, and production. Red Hat Enterprise Linux subscriptions would cover the systems running our code repository and CI/CD pipelines, because our production code is flowing through them. Like the Developer Subscription for Teams, most production Red Hat Enterprise Linux subscriptions are cloud-eligible, meaning that these systems can run on-premise or off, and in the case of the CI/CD pipelines, can be spun up in a cloud on-demand to support our deployment activities.&lt;/p&gt; &lt;h2&gt;Monitor&lt;/h2&gt; &lt;p&gt;These systems are the watchers on the (proverbial) wall: constantly checking in on our various production and pre-production systems, ensuring they’re up and running as expected. Should a system go down or become unresponsive, these systems are responsible for identifying the downed system(s), attempting to recover services automatically, and, if that fails, reaching out for human intervention to restore functionality via various communication systems such as email or chat notifications. The systems watching for outages, attempting automatic recovery, and transporting outage notifications would consume production Red Hat Enterprise Linux subscriptions.&lt;/p&gt; &lt;h2&gt;Update and maintain&lt;/h2&gt; &lt;p&gt;The last block called out in Figure 1 encompasses systems used to keep our production environment running and healthy. These could be systems dedicated to storing updates for our systems (such as Red Hat Satellite) as well as systems used to apply these updates across our landscape (such as Red Hat Ansible Automation Platform). Other systems used to support these actions, such as bastion servers, proxies for downloading updates, and systems hosting firmware updates or other update packages, would also fall into this category, leveraging a production Red Hat Enterprise Linux subscription.&lt;/p&gt; &lt;p&gt;For a more condensed view of various systems and their respective subscription, here’s a quick reference table:&lt;/p&gt; &lt;div&gt; &lt;table cellspacing="0"&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; &lt;p&gt;&lt;strong&gt;System purpose&lt;/strong&gt;&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;&lt;strong&gt;Subscription type&lt;/strong&gt;&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;p&gt;Code validation and testing system&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Developer Subscription for Teams&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;p&gt;Load generating server for testing&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Developer Subscription for Teams&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;p&gt;Testing database with production data&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Developer Subscription for Teams&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;p&gt;Build server that creates app RPMs&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Developer Subscription for Teams&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;p&gt;API endpoint testing system&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Developer Subscription for Teams&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;p&gt;Outage email server&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Production subscription&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;p&gt;Code repository&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Production subscription&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;p&gt;CI/CD pipeline systems&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Production subscription&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;p&gt;Firmware download proxy&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Production subscription&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The Developer Subscription for Teams is a powerful option for software development and testing in an organization, allowing for easy consumption of Red Hat Enterprise Linux in the spirit of building and testing new and existing applications.&lt;/p&gt; &lt;p&gt;If you’re interested in the Developer Subscription for Teams, &lt;a href="https://www.redhat.com/en/contact?sc_cid=7013a000003163VAAQ"&gt;reach out to a Red Hatter today&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/06/what-qualifies-red-hat-developer-subscription-teams" title="What qualifies for Red Hat Developer Subscription for Teams?"&gt;What qualifies for Red Hat Developer Subscription for Teams?&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Josh Swanson, Brian Gollaher</dc:creator><dc:date>2022-07-06T07:00:00Z</dc:date></entry></feed>
